{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from os.path import join\n",
    "import pickle as pkl\n",
    "import json\n",
    "import torch\n",
    "import torch as th\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from easydict import EasyDict as edict\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 72\n",
    "plt.rcParams['figure.figsize'] = [6.0, 4.0]\n",
    "plt.rcParams['figure.edgecolor'] = (1, 1, 1, 0)\n",
    "plt.rcParams['figure.facecolor'] = (1, 1, 1, 0)\n",
    "# vector graphics type\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: skimage.transform is not available. Will use scipy.misc.imresize instead.\n",
      "Warning: skimage.transform is not available. Will use scipy.misc.imresize instead.\n"
     ]
    }
   ],
   "source": [
    "import circuit_toolkit\n",
    "# print(circuit_toolkit.__file__)\n",
    "from circuit_toolkit.layer_hook_utils import print_specific_layer, get_module_name_shapes, featureFetcher_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/mini_edm\")\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/SiT\")\n",
    "# sys.path.append(\"/n/home12/binxuwang/Github/DiT\")\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/DiffusionReasoning\")\n",
    "# from train_edm import create_model, edm_sampler, EDM\n",
    "# from edm_utils import edm_sampler_inpaint, create_edm, get_default_config\n",
    "# from rule_utils import get_rule_img, get_obj_list, get_rule_list, check_consistent\n",
    "from dataset_utils import train_data2attr_tsr,load_raw_data,load_PGM_abstract\n",
    "from rule_new_utils import check_r3_r2_batch, infer_rule_from_sample_batch, compute_rule_statistics\n",
    "from models import SiT_models, SiT\n",
    "# from download import find_model\n",
    "from transport import create_transport, Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in SiT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SiT_configs = {\n",
    "    \"SiT_XL_1\": {\"depth\": 28, \"hidden_size\": 1152, \"patch_size\": 1, \"num_heads\": 16},\n",
    "    \"SiT_XL_3\": {\"depth\": 28, \"hidden_size\": 1152, \"patch_size\": 3, \"num_heads\": 16},\n",
    "    \"SiT_L_1\": {\"depth\": 24, \"hidden_size\": 1024, \"patch_size\": 1, \"num_heads\": 16},\n",
    "    \"SiT_L_3\": {\"depth\": 24, \"hidden_size\": 1024, \"patch_size\": 3, \"num_heads\": 16},\n",
    "    \"SiT_B_1\": {\"depth\": 12, \"hidden_size\": 768, \"patch_size\": 1, \"num_heads\": 12},\n",
    "    \"SiT_B_3\": {\"depth\": 12, \"hidden_size\": 768, \"patch_size\": 3, \"num_heads\": 12},\n",
    "    \"SiT_S_1\": {\"depth\": 12, \"hidden_size\": 384, \"patch_size\": 1, \"num_heads\": 6},\n",
    "    \"SiT_S_3\": {\"depth\": 12, \"hidden_size\": 384, \"patch_size\": 3, \"num_heads\": 6},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dropout_prob = 1.0\n",
    "num_classes = 0\n",
    "model_cfg = SiT_configs[\"SiT_S_1\"]\n",
    "model_SiT = SiT(\n",
    "        input_size=9,\n",
    "        in_channels=3,\n",
    "        num_classes=num_classes,\n",
    "        class_dropout_prob=class_dropout_prob,\n",
    "        learn_sigma=True,\n",
    "        **model_cfg,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "004-SiT_S_1-stream0_16M_pilot-Linear-velocity-None\n",
      "005-SiT_B_1-stream0_16M_pilot-Linear-velocity-None\n",
      "006-SiT_S_1-stream0_016M_all-Linear-velocity-None\n",
      "007-SiT_S_1-stream0_16M_all-Linear-velocity-None\n",
      "008-SiT_S_1-stream1_6M_all-Linear-velocity-None\n",
      "009-SiT_S_1-stream16M_all-Linear-velocity-None\n",
      "010-SiT_S_1-stream16M_all-Linear-velocity-None\n",
      "011-SiT_B_1-stream0_16M_all-Linear-velocity-None\n",
      "011-SiT_S_1-stream16M_heldout0-Linear-velocity-None\n",
      "012-SiT_B_1-stream0_016M_all-Linear-velocity-None\n",
      "012-SiT_S_1-stream1_6M_heldout0-Linear-velocity-None\n",
      "013-SiT_B_1-stream16M_all-Linear-velocity-None\n",
      "013-SiT_S_1-stream0_016M_heldout0-Linear-velocity-None\n",
      "014-SiT_B_1-stream1_6M_all-Linear-velocity-None\n",
      "014-SiT_S_1-stream0_16M_heldout0-Linear-velocity-None\n",
      "015-SiT_B_1-stream0_016M_heldout0-Linear-velocity-None\n",
      "016-SiT_B_1-stream16M_heldout0-Linear-velocity-None\n",
      "017-SiT_B_1-stream0_16M_heldout0-Linear-velocity-None\n",
      "018-SiT_B_1-stream1_6M_heldout0-Linear-velocity-None\n",
      "019-SiT_B_1-stream16M_all-Linear-velocity-None\n",
      "020-SiT_B_1-stream0_16M_all-Linear-velocity-None\n",
      "021-SiT_B_1-stream1_6M_all-Linear-velocity-None\n",
      "022-SiT_B_1-stream0_016M_all-Linear-velocity-None\n"
     ]
    }
   ],
   "source": [
    "!ls /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/SiT/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exproot = r\"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/SiT/results\"\n",
    "expname = r\"014-SiT_S_1-stream0_16M_heldout0-Linear-velocity-None\"\n",
    "expdir = join(exproot, expname)\n",
    "ckptdir = join(expdir, \"checkpoints\")\n",
    "ckpt_path = join(ckptdir, \"1000000.pt\")\n",
    "state_dict = th.load(ckpt_path, )\n",
    "model_SiT.load_state_dict(state_dict['ema']) # \"model\"\n",
    "model_SiT.to(device).eval();\n",
    "config = edict(json.load(open(join(expdir, \"args.json\"), \"r\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract RAVEN dataset\n",
    "dataset_Xmean = th.tensor([1.5, 2.5, 2.5]).view(1, 3, 1, 1).to(\"cuda\")\n",
    "dataset_Xstd = th.tensor([2.5, 3.5, 3.5]).view(1, 3, 1, 1).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport = create_transport(\n",
    "    config.path_type, #\"Linear\",\n",
    "    config.prediction, #\"velocity\", \n",
    "    config.loss_weight, #None, \n",
    "    config.train_eps, #None,\n",
    "    config.sample_eps, #None,\n",
    ")  # default: velocity; \n",
    "transport_sampler = Sampler(transport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3: 1447/2048 (0.71), C3 + C2: 1720/2048 (0.84), AnyValid: 5196/6144 (0.85)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2048\n",
    "zs = th.randn(batch_size, 3, 9, 9).to(device)\n",
    "ys = torch.zeros((batch_size,), device=device, dtype=torch.int)    \n",
    "sample_model_kwargs = dict(y=ys)\n",
    "model_fn = model_SiT.forward\n",
    "sample_fn = transport_sampler.sample_ode() # default to ode sampling\n",
    "with th.no_grad():\n",
    "    samples = sample_fn(zs, model_fn, **sample_model_kwargs)[-1] # takes 75 sec, pretty slow...\n",
    "samples = ((samples.detach() * dataset_Xstd) + dataset_Xmean).cpu()\n",
    "r3_list, r2_list, rule_col = infer_rule_from_sample_batch(samples)\n",
    "C3_count, C2_count, anyvalid_count, total = compute_rule_statistics(r3_list, r2_list, rule_col, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampler comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_method_set = {\"dopri8\", \"dopri5\", \"bosh3\", \"fehlberg2\", \"adaptive_heun\", \"euler\", \"midpoint\", \"rk4\", \"explicit_adams\", \"implicit_adams\", \"fixed_adams\", \"scipy_solver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3: 1276/2048 (0.62), C3 + C2: 1601/2048 (0.78), AnyValid: 4878/6144 (0.79)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2048\n",
    "zs = th.randn(batch_size, 3, 9, 9).to(device)\n",
    "ys = torch.zeros((batch_size,), device=device, dtype=torch.int)    \n",
    "sample_model_kwargs = dict(y=ys)\n",
    "model_fn = model_SiT.forward\n",
    "sample_fn = transport_sampler.sample_ode(sampling_method=\"euler\", num_steps=50,) # default to ode sampling\n",
    "with th.no_grad():\n",
    "    samples = sample_fn(zs, model_fn, **sample_model_kwargs)[-1] # takes 26 sec, pretty slow...\n",
    "samples = ((samples.detach() * dataset_Xstd) + dataset_Xmean).cpu()\n",
    "r3_list, r2_list, rule_col = infer_rule_from_sample_batch(samples)\n",
    "C3_count, C2_count, anyvalid_count, total = compute_rule_statistics(r3_list, r2_list, rule_col, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3: 1397/2048 (0.68), C3 + C2: 1680/2048 (0.82), AnyValid: 5106/6144 (0.83)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2048\n",
    "zs = th.randn(batch_size, 3, 9, 9).to(device)\n",
    "ys = torch.zeros((batch_size,), device=device, dtype=torch.int)    \n",
    "sample_model_kwargs = dict(y=ys)\n",
    "model_fn = model_SiT.forward\n",
    "sample_fn = transport_sampler.sample_ode(sampling_method=\"euler\", num_steps=100,) # default to ode sampling\n",
    "with th.no_grad():\n",
    "    samples = sample_fn(zs, model_fn, **sample_model_kwargs)[-1] # takes 26 sec, pretty slow...\n",
    "samples = ((samples.detach() * dataset_Xstd) + dataset_Xmean).cpu()\n",
    "r3_list, r2_list, rule_col = infer_rule_from_sample_batch(samples)\n",
    "C3_count, C2_count, anyvalid_count, total = compute_rule_statistics(r3_list, r2_list, rule_col, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3: 1438/2048 (0.70), C3 + C2: 1706/2048 (0.83), AnyValid: 5161/6144 (0.84)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2048\n",
    "zs = th.randn(batch_size, 3, 9, 9).to(device)\n",
    "ys = torch.zeros((batch_size,), device=device, dtype=torch.int)    \n",
    "sample_model_kwargs = dict(y=ys)\n",
    "model_fn = model_SiT.forward\n",
    "sample_fn = transport_sampler.sample_ode(sampling_method=\"adaptive_heun\", num_steps=50,) # default to ode sampling\n",
    "with th.no_grad():\n",
    "    samples = sample_fn(zs, model_fn, **sample_model_kwargs)[-1] # takes 2min, pretty slow...\n",
    "samples = ((samples.detach() * dataset_Xstd) + dataset_Xmean).cpu()\n",
    "r3_list, r2_list, rule_col = infer_rule_from_sample_batch(samples)\n",
    "C3_count, C2_count, anyvalid_count, total = compute_rule_statistics(r3_list, r2_list, rule_col, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3: 1435/2048 (0.70), C3 + C2: 1709/2048 (0.83), AnyValid: 5176/6144 (0.84)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2048\n",
    "zs = th.randn(batch_size, 3, 9, 9).to(device)\n",
    "ys = torch.zeros((batch_size,), device=device, dtype=torch.int)    \n",
    "sample_model_kwargs = dict(y=ys)\n",
    "model_fn = model_SiT.forward\n",
    "sample_fn = transport_sampler.sample_ode(sampling_method=\"rk4\", num_steps=50,) # default to ode sampling\n",
    "with th.no_grad():\n",
    "    samples = sample_fn(zs, model_fn, **sample_model_kwargs)[-1] # takes 2min, pretty slow...\n",
    "samples = ((samples.detach() * dataset_Xstd) + dataset_Xmean).cpu()\n",
    "r3_list, r2_list, rule_col = infer_rule_from_sample_batch(samples)\n",
    "C3_count, C2_count, anyvalid_count, total = compute_rule_statistics(r3_list, r2_list, rule_col, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3: 0/2048 (0.00), C3 + C2: 0/2048 (0.00), AnyValid: 0/6144 (0.00)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2048\n",
    "zs = th.randn(batch_size, 3, 9, 9).to(device)\n",
    "ys = torch.zeros((batch_size,), device=device, dtype=torch.int)    \n",
    "sample_model_kwargs = dict(y=ys)\n",
    "model_fn = model_SiT.forward\n",
    "sample_fn = transport_sampler.sample_sde() # default to ode sampling\n",
    "with th.no_grad():\n",
    "    samples = sample_fn(zs, model_fn, **sample_model_kwargs)[-1]  # takes 4 mins, pretty slow... and is worse than ode sampling!! \n",
    "samples = ((samples.detach() * dataset_Xstd) + dataset_Xmean).cpu()\n",
    "r3_list, r2_list, rule_col = infer_rule_from_sample_batch(samples)\n",
    "C3_count, C2_count, anyvalid_count, total = compute_rule_statistics(r3_list, r2_list, rule_col, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
