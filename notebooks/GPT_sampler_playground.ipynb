{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from os.path import join\n",
    "import json\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import einops\n",
    "# Import necessary libraries\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "from tqdm import tqdm, trange\n",
    "# from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/n/home12/binxuwang/Github/DiffusionReasoning\")\n",
    "from GPT_models.GPT_RAVEN_model_lib import MultiIdxGPT2Model, sample_next_token, seqtsr2imgtsr, seqtsr2attrtsr, completion_eval, preprocess_ids\n",
    "from rule_new_utils import infer_rule_from_sample_batch, compute_rule_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_next_token(model, prefix_inputs, max_length=81, strategy=\"greedy\", device=\"cuda\", temperature=1.0, cond=None):\n",
    "    prefix_inputs = prefix_inputs.to(device)\n",
    "    model.eval().to(device)\n",
    "    prefix_length = prefix_inputs.size(1)\n",
    "    for i in range(max_length - prefix_length):\n",
    "        outputs, logits1, logits2, logits3 = model(prefix_inputs, y=cond)\n",
    "        if strategy == \"greedy\":\n",
    "            next_token1 = torch.argmax(logits1[:, -1, :], dim=-1, keepdim=True)\n",
    "            next_token2 = torch.argmax(logits2[:, -1, :], dim=-1, keepdim=True)\n",
    "            next_token3 = torch.argmax(logits3[:, -1, :], dim=-1, keepdim=True)\n",
    "        elif strategy == \"sample\":\n",
    "            next_token1 = torch.multinomial(F.softmax(logits1[:, -1, :] / temperature, dim=-1), num_samples=1)\n",
    "            next_token2 = torch.multinomial(F.softmax(logits2[:, -1, :] / temperature, dim=-1), num_samples=1)\n",
    "            next_token3 = torch.multinomial(F.softmax(logits3[:, -1, :] / temperature, dim=-1), num_samples=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid strategy\")\n",
    "        next_token = torch.cat([next_token1, next_token2, next_token3], dim=-1)\n",
    "        prefix_inputs = torch.cat([prefix_inputs, next_token[:,None,:]], dim=1)\n",
    "    return prefix_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def completion_eval(eval_samples, model, cond=None, device='cuda', num_mask=9, batch_size=512, \n",
    "                    strategy=\"greedy\", temperature=1.0, return_stats=False):\n",
    "    eval_samples = eval_samples.to(device)\n",
    "    if batch_size is None:\n",
    "        batch_size = eval_samples.size(0)\n",
    "    eval_complete = []\n",
    "    for idx in trange(0, eval_samples.size(0), batch_size):\n",
    "        eval_batch = eval_samples[idx:idx+batch_size]\n",
    "        cond_batch = cond[idx:idx+batch_size] if cond is not None else None\n",
    "        eval_complete_batch = sample_next_token(model, eval_batch[:,:-num_mask,:], temperature=temperature,\n",
    "                                          max_length=81, strategy=strategy, device=device, cond=cond_batch).cpu()\n",
    "        eval_complete.append(eval_complete_batch)\n",
    "        \n",
    "    eval_complete = torch.cat(eval_complete, dim=0)\n",
    "    # eval_complete = sample_next_token(model, eval_samples[:,:-num_mask,:], \n",
    "    #                                   max_length=81, strategy=strategy, device=device).cpu()\n",
    "    # eval_complete_attr = seqtsr2attrtsr(eval_complete, h=3, w=3, p=3, R=3)\n",
    "    # note we need to denormalize, offset by - 1\n",
    "    eval_complete = eval_complete - 1\n",
    "    eval_complete_img = seqtsr2imgtsr(eval_complete, h=3, w=3, p=3, R=3)\n",
    "    C3_list, C2_list, rule_col_list = infer_rule_from_sample_batch(eval_complete_img)\n",
    "    C3_count, C2_count, anyvalid_count, total = compute_rule_statistics(C3_list, C2_list, rule_col_list)\n",
    "    # final_row = np.array(rule_col_list, dtype=object)[:,-1]\n",
    "    # anyvalid_count = sum([len(x) > 0 for x in final_row])\n",
    "    print(f\"Completion: C3: {C3_count / total:.3f} [{C3_count}/{total}],  valid: {anyvalid_count / total / 3:.3f} [{anyvalid_count}/{total*3}]\")\n",
    "    if return_stats:\n",
    "        return eval_complete, C3_list, C2_list, rule_col_list, {\"C3\": C3_count, \"C2\": C2_count, \"anyvalid\": anyvalid_count, \"total\": total}\n",
    "    else:\n",
    "        return eval_complete, C3_list, C2_list, rule_col_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_next_token(model, prefix_inputs, max_length=81, strategy=\"greedy\", device=\"cuda\", \n",
    "                      temperature=1.0, beam_width=3, cond=None):\n",
    "    prefix_inputs = prefix_inputs.to(device)\n",
    "    model.eval().to(device)\n",
    "    prefix_length = prefix_inputs.size(1)\n",
    "    \n",
    "    if strategy == \"beam_search\":\n",
    "        # Initialize beams with the prefix inputs and their scores\n",
    "        beams = [(prefix_inputs, 0)]  # List of (sequence, score)\n",
    "        \n",
    "        for i in range(max_length - prefix_length):\n",
    "            new_beams = []\n",
    "            for seq, score in beams:\n",
    "                outputs, logits1, logits2, logits3 = model(seq, y=cond)\n",
    "                # Compute probabilities and take top beam_width tokens\n",
    "                logits_combined = torch.cat([logits1[:, -1, :], logits2[:, -1, :], logits3[:, -1, :]], dim=-1)\n",
    "                probs = F.softmax(logits_combined / temperature, dim=-1)\n",
    "                top_probs, top_indices = torch.topk(probs, beam_width, dim=-1)\n",
    "                \n",
    "                for j in range(beam_width):\n",
    "                    next_token = top_indices[:, j].unsqueeze(-1)\n",
    "                    new_seq = torch.cat([seq, next_token[:, None, :]], dim=1)\n",
    "                    new_score = score + torch.log(top_probs[:, j]).item()\n",
    "                    new_beams.append((new_seq, new_score))\n",
    "            \n",
    "            # Sort all new beams and keep the top beam_width\n",
    "            new_beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            beams = new_beams\n",
    "        \n",
    "        # Return the sequence with the highest score\n",
    "        best_seq = max(beams, key=lambda x: x[1])[0]\n",
    "        return best_seq\n",
    "    \n",
    "    else:\n",
    "        # Greedy or sampling strategies (existing code)\n",
    "        for i in range(max_length - prefix_length):\n",
    "            outputs, logits1, logits2, logits3 = model(prefix_inputs, y=cond)\n",
    "            if strategy == \"greedy\":\n",
    "                next_token1 = torch.argmax(logits1[:, -1, :], dim=-1, keepdim=True)\n",
    "                next_token2 = torch.argmax(logits2[:, -1, :], dim=-1, keepdim=True)\n",
    "                next_token3 = torch.argmax(logits3[:, -1, :], dim=-1, keepdim=True)\n",
    "            elif strategy == \"sample\":\n",
    "                next_token1 = torch.multinomial(F.softmax(logits1[:, -1, :] / temperature, dim=-1), num_samples=1)\n",
    "                next_token2 = torch.multinomial(F.softmax(logits2[:, -1, :] / temperature, dim=-1), num_samples=1)\n",
    "                next_token3 = torch.multinomial(F.softmax(logits3[:, -1, :] / temperature, dim=-1), num_samples=1)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid strategy\")\n",
    "            next_token = torch.cat([next_token1, next_token2, next_token3], dim=-1)\n",
    "            prefix_inputs = torch.cat([prefix_inputs, next_token[:, None, :]], dim=1)\n",
    "        return prefix_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabdir = \"/n/home12/binxuwang/Github/DiffusionReasoning/Tables\"\n",
    "figdir = \"/n/home12/binxuwang/Github/DiffusionReasoning/Figures_newrule\"\n",
    "\n",
    "GPT_exproot = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/GPT2_raven\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2_base_pilot\n",
      "GPT2_base_pilot_fixed\n",
      "GPT2_base_RAVEN_cond_heldout0-20240701-225339\n",
      "GPT2_base_RAVEN_uncond_heldout0-20240515-021155\n",
      "GPT2_base_RAVEN_uncond_heldout0-20240630-023945\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream0_016M-20240819-010518\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream0_016M-20240820-020027\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream0_016M-20240820-023934\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream0_16M-20240819-032725\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream0_16M-20240820-021649\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream0_16M-20240820-024013\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream1_6M-20240818-012450\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream1_6M-20240818-013524\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream1_6M-20240818-014017\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream16M-20240819-010517\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream1_6M-20240819-010517\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream16M-20240820-020037\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream1_6M-20240820-020037\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream1_6M-20240820-023925\n",
      "GPT2_base_RAVEN_uncond_heldout0_stream16M-20240820-024031\n",
      "GPT2_big_pilot_fixed\n",
      "GPT2_big_RAVEN_uncond_heldout0-20240515-085510\n",
      "GPT2CmbEmb_base_RAVEN_uncond_heldout0-20240701-154321\n",
      "GPT2_medium_RAVEN_cond_all-20240702-032839\n",
      "GPT2_medium_RAVEN_cond_all_stream16M-20240704-030228\n",
      "GPT2_medium_RAVEN_cond_heldout0-20240701-231350\n",
      "GPT2_medium_RAVEN_uncond_all-20240702-032428\n",
      "GPT2_medium_RAVEN_uncond_all_stream16M-20240704-024215\n",
      "GPT2_medium_RAVEN_uncond_all_stream16M-20240704-025623\n",
      "GPT2_medium_RAVEN_uncond_heldout0-20240701-133418\n",
      "GPT2_medium_RAVEN_uncond_heldout0_stream0_016M-20240819-010618\n",
      "GPT2_medium_RAVEN_uncond_heldout0_stream0_016M-20240820-020055\n",
      "GPT2_medium_RAVEN_uncond_heldout0_stream0_016M-20240820-024735\n",
      "GPT2_medium_RAVEN_uncond_heldout0_stream0_16M-20240819-011524\n",
      "GPT2_medium_RAVEN_uncond_heldout0_stream0_16M-20240820-020055\n",
      "GPT2_medium_RAVEN_uncond_heldout0_stream0_16M-20240820-024019\n",
      "GPT2_medium_RAVEN_uncond_heldout0_stream16M-20240704-041344\n",
      "GPT2_medium_RAVEN_uncond_heldout0_stream16M-20240819-010648\n",
      "GPT2_medium_RAVEN_uncond_heldout0_stream1_6M-20240819-010648\n",
      "GPT2_medium_RAVEN_uncond_heldout0_stream1_6M-20240820-023917\n",
      "GPT2_medium_RAVEN_uncond_heldout0_stream16M-20240820-024718\n",
      "GPT2_small_RAVEN_uncond_heldout0_stream0_016M-20240820-020243\n",
      "GPT2_small_RAVEN_uncond_heldout0_stream0_016M-20240820-024539\n",
      "GPT2_small_RAVEN_uncond_heldout0_stream0_16M-20240820-020139\n",
      "GPT2_small_RAVEN_uncond_heldout0_stream0_16M-20240820-024128\n",
      "GPT2_small_RAVEN_uncond_heldout0_stream1_6M-20240818-013943\n",
      "GPT2_small_RAVEN_uncond_heldout0_stream1_6M-20240820-024123\n",
      "GPT2_small_RAVEN_uncond_heldout0_stream16M-20240820-024127\n"
     ]
    }
   ],
   "source": [
    "!ls {GPT_exproot}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "expname = \"GPT2_medium_RAVEN_uncond_heldout0_stream0_16M-20240820-024019\"\n",
    "expdir = join(GPT_exproot, expname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ckpt  config.json  repr_classifier  samples  tensorboard_logs\n"
     ]
    }
   ],
   "source": [
    "!ls {expdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'epoch_total': 457,\n",
       " 'save_ckpt_every_step': 25000,\n",
       " 'eval_model_every_step': 2500,\n",
       " 'lr': 0.0001,\n",
       " 'num_warmup_steps': 100,\n",
       " 'n_embd': 768,\n",
       " 'n_class': 0,\n",
       " 'n_layer': 24,\n",
       " 'n_head': 12,\n",
       " 'is_sep_embed': True,\n",
       " 'heldout_id': [1, 16, 20, 34, 37],\n",
       " 'train_sample_num': 140000,\n",
       " 'val_sample_num': 20000,\n",
       " 'eval_temperature': 1.0,\n",
       " 'is_class_cond': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = json.load(open(join(expdir, 'config.json')))\n",
    "config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpt2_raven(expname, ckpt_step=999999):\n",
    "    expdir = join(GPT_exproot, expname)\n",
    "    config = json.load(open(join(expdir, 'config.json')))\n",
    "    gpt2_raven = MultiIdxGPT2Model(attribute_dims=(7,10,10), vocab_size=27, max_length=83, n_embd=config['n_embd'],\n",
    "                               n_class=config['n_class'], n_head=config['n_head'], n_layer=config['n_layer'])\n",
    "    gpt2_raven.load_state_dict(th.load(join(expdir, 'ckpt', f'gpt2_step{ckpt_step}.pth')))\n",
    "    gpt2_raven.to('cuda').eval()\n",
    "    return gpt2_raven\n",
    "\n",
    "expname = \"GPT2_medium_RAVEN_uncond_heldout0_stream0_16M-20240820-024019\"\n",
    "ckpt_step = 999999\n",
    "gpt2_raven = load_gpt2_raven(expname, ckpt_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ab initio generation, sampling: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612a76a84e23478db3888016521357cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion: C3: 0.357 [183/512],  valid: 0.625 [960/1536]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ab initio generation, sampling: \")\n",
    "# rnd_idx = np.random.choice(len(attr_seq_tsr_val), 512)\n",
    "# eval_samples = attr_seq_tsr_val[rnd_idx,:,:]\n",
    "eval_samples_empty = torch.zeros(512, 81, 3, dtype=th.long).to('cuda')\n",
    "eval_complete, C3_list, C2_list, rule_col_list = completion_eval(eval_samples_empty, gpt2_raven, num_mask=81, \n",
    "                                                                 device='cuda', strategy=\"sample\", batch_size=512)\n",
    "th.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/wouterkool/stochastic-beam-search/tree/stochastic-beam-search\n",
    "\n",
    "https://github.com/evanthebouncy/stoicastic_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_next_token_beam_search(\n",
    "    model, prefix_inputs, max_length=81, beam_size=5, device=\"cuda\", cond=None, strategy=\"topk_beam\", temperature=1.0, return_best=True\n",
    "):\n",
    "    prefix_inputs = prefix_inputs.to(device)  # shape [1, seq_len, 3]\n",
    "    batch_size = prefix_inputs.size(0)\n",
    "    prefix_length = prefix_inputs.size(1)\n",
    "    if batch_size != 1:\n",
    "        raise NotImplementedError(\"Beam search for batch size >1 not implemented.\")\n",
    "    model.eval().to(device)\n",
    "    # Initialize the beam with the prefix input and zero cumulative log probability\n",
    "    beam = [(prefix_inputs, 0.0)]  # list of tuples (sequence tensor, cumulative_log_prob)\n",
    "    current_beam_size = 1 # number of sequences in the beam, initialized to 1\n",
    "    for _ in range(max_length - prefix_length):\n",
    "        # Collect sequences and cumulative_log_probs from the current beam\n",
    "        beam_sequences = torch.cat([seq for seq, _ in beam], dim=0)  # [beam_size, seq_len, 3]\n",
    "        beam_log_probs = torch.tensor(\n",
    "            [log_prob for _, log_prob in beam], device=device\n",
    "        )  # [beam_size]\n",
    "\n",
    "        # Run the model on all sequences in the beam\n",
    "        outputs, logits1, logits2, logits3 = model(beam_sequences, y=cond)\n",
    "        logits1 = logits1[:, -1, :] / temperature # [beam_size, vocab_size1]\n",
    "        logits2 = logits2[:, -1, :] / temperature # [beam_size, vocab_size2]\n",
    "        logits3 = logits3[:, -1, :] / temperature # [beam_size, vocab_size3]\n",
    "\n",
    "        V1, V2, V3 = logits1.size(-1), logits2.size(-1), logits3.size(-1)\n",
    "        total_vocab_size = V1 * V2 * V3\n",
    "\n",
    "        # Compute log probabilities\n",
    "        log_probs1 = F.log_softmax(logits1, dim=-1)  # [beam_size, V1]\n",
    "        log_probs2 = F.log_softmax(logits2, dim=-1)  # [beam_size, V2]\n",
    "        log_probs3 = F.log_softmax(logits3, dim=-1)  # [beam_size, V3]\n",
    "\n",
    "        # Compute joint log probabilities for all combinations\n",
    "        log_probs1_exp = log_probs1.unsqueeze(2).unsqueeze(3)  # [beam_size, V1, 1, 1]\n",
    "        log_probs2_exp = log_probs2.unsqueeze(1).unsqueeze(3)  # [beam_size, 1, V2, 1]\n",
    "        log_probs3_exp = log_probs3.unsqueeze(1).unsqueeze(2)  # [beam_size, 1, 1, V3]\n",
    "        joint_log_probs = (\n",
    "            log_probs1_exp + log_probs2_exp + log_probs3_exp\n",
    "        )  # [beam_size, V1, V2, V3]\n",
    "\n",
    "        # Flatten joint log probabilities\n",
    "        joint_log_probs = joint_log_probs.view(current_beam_size, -1)  # [beam_size, V1*V2*V3]\n",
    "\n",
    "        # Compute cumulative log probabilities\n",
    "        cumulative_log_probs = (\n",
    "            beam_log_probs.unsqueeze(1) + joint_log_probs\n",
    "        )  # [beam_size, V1*V2*V3]\n",
    "\n",
    "        # Flatten for selecting top candidates, combinatoric of [ old beams X new tokens ] \n",
    "        cumulative_log_probs_flat = cumulative_log_probs.view(-1)  # [beam_size * V1*V2*V3]\n",
    "\n",
    "        if strategy == \"topk_beam\":\n",
    "            # Get top beam_size sequences\n",
    "            sampled_log_probs, sampled_indices = torch.topk(cumulative_log_probs_flat, k=beam_size)\n",
    "        # elif strategy == \"sample_beam\":\n",
    "        #     cumulative_probs_flat = torch.exp(cumulative_log_probs_flat - cumulative_log_probs_flat.max())\n",
    "        #     cumulative_probs_flat /= cumulative_probs_flat.sum()\n",
    "        #     sampled_indices = torch.multinomial(cumulative_probs_flat, num_samples=beam_size, replacement=False)\n",
    "        #     sampled_log_probs = cumulative_log_probs_flat[sampled_indices]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid strategy: {strategy}\")\n",
    "        \n",
    "        new_beam = []\n",
    "        for idx in range(beam_size):\n",
    "            log_prob = sampled_log_probs[idx]\n",
    "            flat_index = sampled_indices[idx]\n",
    "            beam_idx = flat_index // total_vocab_size\n",
    "            vocab_idx = flat_index % total_vocab_size\n",
    "            seq = beam_sequences[beam_idx]  # [seq_len, 3]\n",
    "            # Decode indices for the next token's attributes\n",
    "            idx1 = vocab_idx // (V2 * V3)\n",
    "            idx2 = (vocab_idx % (V2 * V3)) // V3\n",
    "            idx3 = vocab_idx % V3\n",
    "            # Create the next token\n",
    "            next_token = torch.tensor(\n",
    "                [[idx1.item(), idx2.item(), idx3.item()]], device=device\n",
    "            ).unsqueeze(0)  # [1, 1, 3]\n",
    "            # Append the next token to the sequence\n",
    "            new_seq = torch.cat([seq.unsqueeze(0), next_token], dim=1)  # [1, seq_len+1, 3]\n",
    "            new_beam.append((new_seq, log_prob))\n",
    "        current_beam_size = len(new_beam)\n",
    "        # Update the beam with new sequences\n",
    "        beam = new_beam\n",
    "    \n",
    "    if return_best:\n",
    "        # Return the sequence with the highest cumulative log probability\n",
    "        best_seq = beam[0][0]  # [1, max_length, 3]\n",
    "        return best_seq.squeeze(0)  # Remove the batch dimension\n",
    "    else:\n",
    "        seqs = torch.cat([seq for seq, _ in beam], dim=0)\n",
    "        scores = torch.tensor([log_prob for _, log_prob in beam], device=device)\n",
    "        return seqs, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import gumbel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.66904392, -0.30546029,  2.58577679,  1.21405472,  1.49607145,\n",
       "        0.80941592,  0.10930291,  1.53330812, -0.77075769, -0.66470453])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gumbel(loc=np.zeros(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from numpy.random import gumbel # \n",
    "# torch version gumbel \n",
    "from torch.distributions.gumbel import Gumbel\n",
    "\n",
    "def sample_gumbel(shape, device, eps=1e-20):\n",
    "    \"\"\"Sample Gumbel noise.\"\"\"\n",
    "    U = torch.rand(shape, device=device)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_next_token_stochastic_beam_search(\n",
    "    model, prefix_inputs, max_length=81, beam_size=5, device=\"cuda\", cond=None, strategy=\"gumbel_topk_beam\", temperature=1.0, return_best=True\n",
    "):\n",
    "    prefix_inputs = prefix_inputs.to(device)  # shape [1, seq_len, 3]\n",
    "    batch_size = prefix_inputs.size(0)\n",
    "    prefix_length = prefix_inputs.size(1)\n",
    "    if batch_size != 1:\n",
    "        raise NotImplementedError(\"Beam search for batch size >1 not implemented.\")\n",
    "    model.eval().to(device)\n",
    "    # Initialize the beam with the prefix input and zero cumulative log probability\n",
    "    beam = [(prefix_inputs, 0.0, 0.0)]  # list of tuples (sequence tensor, cumulative_log_prob)\n",
    "    current_beam_size = 1 # number of sequences in the beam, initialized to 1\n",
    "    for _ in range(max_length - prefix_length):\n",
    "        # Collect sequences and cumulative_log_probs from the current beam\n",
    "        beam_sequences = torch.cat([seq for seq, _,  _ in beam], dim=0)  # [beam_size, seq_len, 3]\n",
    "        beam_log_probs = torch.tensor(\n",
    "            [log_prob for _, log_prob, _ in beam], device=device\n",
    "        )  # [beam_size]\n",
    "        beam_gumbel_log_probs = torch.tensor(\n",
    "            [gumbellog_prob for _, _, gumbellog_prob in beam], device=device\n",
    "        )  # [beam_size]\n",
    "        # Run the model on all sequences in the beam\n",
    "        outputs, logits1, logits2, logits3 = model(beam_sequences, y=cond)\n",
    "        logits1 = logits1[:, -1, :] / temperature # [beam_size, vocab_size1]\n",
    "        logits2 = logits2[:, -1, :] / temperature # [beam_size, vocab_size2]\n",
    "        logits3 = logits3[:, -1, :] / temperature # [beam_size, vocab_size3]\n",
    "\n",
    "        V1, V2, V3 = logits1.size(-1), logits2.size(-1), logits3.size(-1)\n",
    "        total_vocab_size = V1 * V2 * V3\n",
    "\n",
    "        # Compute log probabilities\n",
    "        log_probs1 = F.log_softmax(logits1, dim=-1)  # [beam_size, V1]\n",
    "        log_probs2 = F.log_softmax(logits2, dim=-1)  # [beam_size, V2]\n",
    "        log_probs3 = F.log_softmax(logits3, dim=-1)  # [beam_size, V3]\n",
    "\n",
    "        # Compute joint log probabilities for all combinations\n",
    "        log_probs1_exp = log_probs1.unsqueeze(2).unsqueeze(3)  # [beam_size, V1, 1, 1]\n",
    "        log_probs2_exp = log_probs2.unsqueeze(1).unsqueeze(3)  # [beam_size, 1, V2, 1]\n",
    "        log_probs3_exp = log_probs3.unsqueeze(1).unsqueeze(2)  # [beam_size, 1, 1, V3]\n",
    "        joint_log_probs = (\n",
    "            log_probs1_exp + log_probs2_exp + log_probs3_exp\n",
    "        )  # [beam_size, V1, V2, V3]\n",
    "\n",
    "        # Flatten joint log probabilities\n",
    "        joint_log_probs = joint_log_probs.view(current_beam_size, -1)  # [beam_size, V1*V2*V3]\n",
    "\n",
    "        # Compute cumulative log probabilities\n",
    "        cumulative_log_probs = (\n",
    "            beam_log_probs.unsqueeze(1) + joint_log_probs\n",
    "        )  # [beam_size, V1*V2*V3]\n",
    "\n",
    "        gumbel_phi_ss = np.random.gumbel(cumulative_log_probs.cpu().numpy())\n",
    "        gumbel_phi_ss = th.tensor(gumbel_phi_ss, device=device)\n",
    "        # gumbel_phi_ss = cumulative_log_probs + sample_gumbel(cumulative_log_probs.shape, device)\n",
    "        z = th.max(gumbel_phi_ss, dim=1).values\n",
    "        exp_neg_g_phi_s = th.exp(-beam_gumbel_log_probs) # [beam_size]\n",
    "        exp_neg_z = th.exp(-z) # [beam_size]\n",
    "        exp_neg_g_phi_ss = th.exp(-gumbel_phi_ss) # [beam_size, V1*V2*V3]\n",
    "        gumbel_hat_phi_ss = -th.log(exp_neg_g_phi_s[:, None] - exp_neg_z[:, None] + exp_neg_g_phi_ss) # [beam_size, V1*V2*V3]\n",
    "        # Flatten for selecting top candidates, combinatoric of [ old beams X new tokens ] \n",
    "        cumulative_log_probs_flat = cumulative_log_probs.view(-1)  # [beam_size * V1*V2*V3]\n",
    "        gumbel_hat_phi_ss_flat = gumbel_hat_phi_ss.view(-1) # [beam_size * V1*V2*V3]\n",
    "        if strategy == \"gumbel_topk_beam\":\n",
    "            sampled_gumble_hat_log_probs, sampled_indices = torch.topk(gumbel_hat_phi_ss_flat, k=beam_size)\n",
    "            sampled_log_probs = cumulative_log_probs_flat[sampled_indices]\n",
    "        # if strategy == \"topk_beam\":\n",
    "        #     # Get top beam_size sequences\n",
    "        #     sampled_log_probs, sampled_indices = torch.topk(cumulative_log_probs_flat, k=beam_size)\n",
    "        # elif strategy == \"sample_beam\":\n",
    "        #     cumulative_probs_flat = torch.exp(cumulative_log_probs_flat - cumulative_log_probs_flat.max())\n",
    "        #     cumulative_probs_flat /= cumulative_probs_flat.sum()\n",
    "        #     sampled_indices = torch.multinomial(cumulative_probs_flat, num_samples=beam_size, replacement=False)\n",
    "        #     sampled_log_probs = cumulative_log_probs_flat[sampled_indices]\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid strategy: {strategy}\")\n",
    "        \n",
    "        new_beam = []\n",
    "        for idx in range(beam_size):\n",
    "            flat_index = sampled_indices[idx]\n",
    "            log_prob = sampled_log_probs[idx]\n",
    "            gumbel_hat_log_prob = sampled_gumble_hat_log_probs[idx]\n",
    "            beam_idx = flat_index // total_vocab_size\n",
    "            vocab_idx = flat_index % total_vocab_size\n",
    "            seq = beam_sequences[beam_idx]  # [seq_len, 3]\n",
    "            # Decode indices for the next token's attributes\n",
    "            idx1 = vocab_idx // (V2 * V3)\n",
    "            idx2 = (vocab_idx % (V2 * V3)) // V3\n",
    "            idx3 = vocab_idx % V3\n",
    "            # Create the next token\n",
    "            next_token = torch.tensor(\n",
    "                [[idx1.item(), idx2.item(), idx3.item()]], device=device\n",
    "            ).unsqueeze(0)  # [1, 1, 3]\n",
    "            # Append the next token to the sequence\n",
    "            new_seq = torch.cat([seq.unsqueeze(0), next_token], dim=1)  # [1, seq_len+1, 3]\n",
    "            new_beam.append((new_seq, log_prob, gumbel_hat_log_prob))\n",
    "        current_beam_size = len(new_beam)\n",
    "        # Update the beam with new sequences\n",
    "        beam = new_beam\n",
    "    \n",
    "    if return_best:\n",
    "        # Return the sequence with the highest cumulative log probability\n",
    "        best_seq = beam[0][0]  # [1, max_length, 3]\n",
    "        return best_seq.squeeze(0)  # Remove the batch dimension\n",
    "    else:\n",
    "        seqs = torch.cat([seq for seq, _, _ in beam], dim=0)\n",
    "        scores = torch.tensor([log_prob for _, log_prob, _ in beam], device=device)\n",
    "        gumbel_hat_log_probs = torch.tensor([gumbel_hat_log_prob for _, _, gumbel_hat_log_prob in beam], device=device)\n",
    "        return seqs, scores, gumbel_hat_log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 81, 3]) torch.Size([256]) torch.Size([256])\n",
      "C3: 84/256 (0.33), C3 + C2: 142/256 (0.55), AnyValid: 476/768 (0.62)\n",
      "Completion: C3: 0.328 [84/256],  valid: 0.620 [476/768]\n",
      "[[], [], [], [], [22], [], [], [], [], [8], [], [5], [], [], [], [], [], [], [], [], [], [23], [], [], [], [], [], [], [], [8], [], [], [], [], [], [], [], [], [10], [], [], [], [], [25], [], [], [], [26], [], [], [], [], [], [], [26], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [22], [], [], [], [], [], [], [], [], [11], [5], [], [], [], [], [22], [], [], [], [], [6], [15], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [11], [3], [23], [], [], [], [5], [4], [], [], [], [], [11], [6], [], [], [], [], [22], [13], [], [], [], [22], [], [], [], [14], [18], [0], [], [], [3], [11], [], [], [], [], [11], [], [18], [], [12], [], [], [], [], [], [], [], [2], [11], [23], [], [], [28], [28], [4], [15], [2], [], [14], [], [18], [19], [], [13], [], [], [17], [11], [4], [15], [], [0], [], [], [], [], [], [], [4], [10], [7], [], [4], [26], [29], [2], [], [28], [6], [28], [], [], [], [], [], [], [2], [], [5], [10], [3], [24], [24], [], [23], [13], [28], [19], [22], [25], [5], [23], [], [], [10], [38], [], [35], [8], [], [], [], [12], [], [17], [27], [0], [], [], [8], [], [10], []]\n"
     ]
    }
   ],
   "source": [
    "seqs, scores, gumbel_hat_log_probs = sample_next_token_stochastic_beam_search(gpt2_raven, eval_samples_empty[0:1, :0], max_length=81, \n",
    "                beam_size=256, device='cuda', strategy=\"gumbel_topk_beam\", temperature=1.0, return_best=False)\n",
    "print(seqs.shape, scores.shape, gumbel_hat_log_probs.shape)\n",
    "\n",
    "eval_complete = seqs.cpu() - 1\n",
    "eval_complete_img = seqtsr2imgtsr(eval_complete, h=3, w=3, p=3, R=3)\n",
    "C3_list, C2_list, rule_col_list = infer_rule_from_sample_batch(eval_complete_img)\n",
    "C3_count, C2_count, anyvalid_count, total = compute_rule_statistics(C3_list, C2_list, rule_col_list)\n",
    "# final_row = np.array(rule_col_list, dtype=object)[:,-1]\n",
    "# anyvalid_count = sum([len(x) > 0 for x in final_row])\n",
    "print(f\"Completion: C3: {C3_count / total:.3f} [{C3_count}/{total}],  valid: {anyvalid_count / total / 3:.3f} [{anyvalid_count}/{total*3}]\") \n",
    "print(C3_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 81, 3])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_samples_empty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8]) torch.Size([1, 1, 11]) torch.Size([1, 1, 11])\n"
     ]
    }
   ],
   "source": [
    "with th.no_grad():\n",
    "    outputs, logits1, logits2, logits3 = gpt2_raven(eval_samples_empty[0:1, :0], y=None)\n",
    "# print(outputs)\n",
    "print(logits1.shape, logits2.shape, logits3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 1 from beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81, 3])\n"
     ]
    }
   ],
   "source": [
    "with th.no_grad():\n",
    "    best_seq = sample_next_token_beam_search(gpt2_raven, eval_samples_empty[0:1, :0], max_length=81, beam_size=10, device='cuda')\n",
    "print(best_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([81, 3])\n"
     ]
    }
   ],
   "source": [
    "with th.no_grad():\n",
    "    best_seq2 = sample_next_token_beam_search(gpt2_raven, eval_samples_empty[0:1, :0], max_length=81, beam_size=5, device='cuda')\n",
    "print(best_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.all(best_seq == best_seq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3: 1/1 (1.00), C3 + C2: 1/1 (1.00), AnyValid: 3/3 (1.00)\n",
      "Completion: C3: 1.000 [1/1],  valid: 1.000 [3/3]\n",
      "[[35]]\n"
     ]
    }
   ],
   "source": [
    "eval_complete = best_seq.cpu().unsqueeze(0) - 1\n",
    "eval_complete_img = seqtsr2imgtsr(eval_complete, h=3, w=3, p=3, R=3)\n",
    "C3_list, C2_list, rule_col_list = infer_rule_from_sample_batch(eval_complete_img)\n",
    "C3_count, C2_count, anyvalid_count, total = compute_rule_statistics(C3_list, C2_list, rule_col_list)\n",
    "# final_row = np.array(rule_col_list, dtype=object)[:,-1]\n",
    "# anyvalid_count = sum([len(x) > 0 for x in final_row])\n",
    "print(f\"Completion: C3: {C3_count / total:.3f} [{C3_count}/{total}],  valid: {anyvalid_count / total / 3:.3f} [{anyvalid_count}/{total*3}]\") \n",
    "print(C3_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top N seq from beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 81, 3]) torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "with th.no_grad():\n",
    "    best_seqs, best_scores = sample_next_token_beam_search(gpt2_raven, eval_samples_empty[0:1, :0], max_length=81, beam_size=10, device='cuda', return_best=False)\n",
    "print(best_seqs.shape, best_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3: 10/10 (1.00), C3 + C2: 10/10 (1.00), AnyValid: 30/30 (1.00)\n",
      "Completion: C3: 1.000 [10/10],  valid: 1.000 [30/30]\n",
      "[[35], [13], [35], [0], [26], [35], [35], [35], [35], [13]]\n",
      "[-21.662336, -21.714895, -21.776728, -23.271214, -23.496033, -23.498383, -31.572052, -31.655994, -32.499695, -34.300903]\n"
     ]
    }
   ],
   "source": [
    "eval_complete = best_seqs.cpu() - 1\n",
    "eval_complete_img = seqtsr2imgtsr(eval_complete, h=3, w=3, p=3, R=3)\n",
    "C3_list, C2_list, rule_col_list = infer_rule_from_sample_batch(eval_complete_img)\n",
    "C3_count, C2_count, anyvalid_count, total = compute_rule_statistics(C3_list, C2_list, rule_col_list)\n",
    "print(f\"Completion: C3: {C3_count / total:.3f} [{C3_count}/{total}],  valid: {anyvalid_count / total / 3:.3f} [{anyvalid_count}/{total*3}]\") \n",
    "print(C3_list)\n",
    "print(list(best_scores.cpu().numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 81, 3]) torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "with th.no_grad():\n",
    "    seqs, scores = sample_next_token_beam_search(gpt2_raven, eval_samples_empty[0:1, :0], max_length=81, beam_size=20, device='cuda', \n",
    "                                             strategy=\"gumbel_topk_beam\", temperature=1.0, return_best=False)\n",
    "print(seqs.shape, scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3: 0/20 (0.00), C3 + C2: 0/20 (0.00), AnyValid: 4/60 (0.07)\n",
      "Completion: C3: 0.000 [0/20],  valid: 0.067 [4/60]\n"
     ]
    }
   ],
   "source": [
    "eval_complete = seqs.cpu() - 1\n",
    "eval_complete_img = seqtsr2imgtsr(eval_complete, h=3, w=3, p=3, R=3)\n",
    "C3_list, C2_list, rule_col_list = infer_rule_from_sample_batch(eval_complete_img)\n",
    "C3_count, C2_count, anyvalid_count, total = compute_rule_statistics(C3_list, C2_list, rule_col_list)\n",
    "print(f\"Completion: C3: {C3_count / total:.3f} [{C3_count}/{total}],  valid: {anyvalid_count / total / 3:.3f} [{anyvalid_count}/{total*3}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2_final.pth\t     gpt2_step399999.pth  gpt2_step724999.pth\n",
      "gpt2_init.pth\t     gpt2_step424999.pth  gpt2_step749999.pth\n",
      "gpt2_step124999.pth  gpt2_step449999.pth  gpt2_step74999.pth\n",
      "gpt2_step149999.pth  gpt2_step474999.pth  gpt2_step774999.pth\n",
      "gpt2_step174999.pth  gpt2_step499999.pth  gpt2_step799999.pth\n",
      "gpt2_step199999.pth  gpt2_step49999.pth   gpt2_step824999.pth\n",
      "gpt2_step224999.pth  gpt2_step524999.pth  gpt2_step849999.pth\n",
      "gpt2_step249999.pth  gpt2_step549999.pth  gpt2_step874999.pth\n",
      "gpt2_step24999.pth   gpt2_step574999.pth  gpt2_step899999.pth\n",
      "gpt2_step274999.pth  gpt2_step599999.pth  gpt2_step924999.pth\n",
      "gpt2_step299999.pth  gpt2_step624999.pth  gpt2_step949999.pth\n",
      "gpt2_step324999.pth  gpt2_step649999.pth  gpt2_step974999.pth\n",
      "gpt2_step349999.pth  gpt2_step674999.pth  gpt2_step999999.pth\n",
      "gpt2_step374999.pth  gpt2_step699999.pth  gpt2_step99999.pth\n"
     ]
    }
   ],
   "source": [
    "!ls {expdir}/ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_step = 999999\n",
    "gpt2_raven = MultiIdxGPT2Model(attribute_dims=(7,10,10), vocab_size=27, max_length=83, n_embd=config['n_embd'],\n",
    "                               n_class=config['n_class'], n_head=config['n_head'], n_layer=config['n_layer'])\n",
    "gpt2_raven.load_state_dict(th.load(join(expdir, 'ckpt', f'gpt2_step{ckpt_step}.pth')))\n",
    "gpt2_raven.to('cuda').eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
