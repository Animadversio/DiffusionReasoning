{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from mamba_ssm import MambaLMHeadModel, Mamba\n",
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "cfg = MambaConfig(d_model=256, n_layer=6, vocab_size=1, \n",
    "                  ssm_cfg={\"layer\": \"Mamba1\"})\n",
    "mamba = MambaLMHeadModel(cfg, device='cuda')\n",
    "mamba.backbone.embedding = nn.Identity()\n",
    "mamba.lm_head = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutput(logits=tensor([[[ 5.1332e-02,  2.6841e-01, -4.1340e+00,  ...,  9.4331e-01,\n",
       "          -3.7225e-01, -1.2420e+00],\n",
       "         [ 3.6157e-05, -5.4496e-01,  6.6071e-02,  ..., -9.6986e-01,\n",
       "           6.7653e-01, -1.5754e+00],\n",
       "         [ 1.7294e+00,  2.9900e-01, -9.3672e-01,  ...,  5.9371e-01,\n",
       "           1.1000e+00,  5.9712e-01],\n",
       "         ...,\n",
       "         [-8.2670e-01,  3.7349e-02, -2.7147e+00,  ..., -1.4583e+00,\n",
       "           1.0624e+00,  1.0814e+00],\n",
       "         [ 1.0152e+00, -1.1093e+00,  5.5335e-01,  ..., -3.9028e-01,\n",
       "          -4.0125e-02, -4.8310e-01],\n",
       "         [-7.3876e-01, -1.9884e-01,  8.9090e-01,  ..., -1.7579e+00,\n",
       "          -4.7668e-01, -6.3906e-01]],\n",
       "\n",
       "        [[ 1.8696e-01,  4.6050e-01, -1.1944e+00,  ..., -6.5685e-01,\n",
       "          -2.5989e-01, -8.5702e-01],\n",
       "         [-6.6320e-01, -5.7837e-01, -1.4769e+00,  ...,  6.1493e-03,\n",
       "          -6.1966e-01,  6.3110e-01],\n",
       "         [-3.5800e-01, -1.1420e+00, -8.5709e-01,  ..., -3.3496e-01,\n",
       "          -2.7078e-01, -4.6696e-01],\n",
       "         ...,\n",
       "         [-2.4447e-01,  1.8210e+00, -4.3476e-01,  ..., -2.9902e-01,\n",
       "           1.1773e+00, -6.6238e-01],\n",
       "         [-9.5350e-01,  2.7978e-01,  1.2946e+00,  ..., -1.3153e+00,\n",
       "          -6.9979e-01, -9.6642e-01],\n",
       "         [-1.6862e-01,  1.1183e+00,  2.6334e-01,  ...,  2.6913e-01,\n",
       "          -2.1029e-01, -5.9705e-01]]], device='cuda:0',\n",
       "       grad_fn=<LayerNormFnBackward>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "mamba(torch.randn(2, 10, 256).float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir  /n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/Mamba_raven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from mamba_ssm import MambaLMHeadModel, Mamba\n",
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "import sys\n",
    "sys.path.append('/n/home12/binxuwang/Github/DiffusionReasoning/')\n",
    "from GPT_models.GPT_RAVEN_model_lib import completion_eval, sample_next_token, \\\n",
    "    multi_attr_loss_vec, MultiIdxGPT2Model, SepWordEmbed, SepLMhead, CmbLMhead, CmbWordEmbed\n",
    "from GPT_models.GPT_RAVEN_model_lib import seqtsr2imgtsr, seqtsr2attrtsr, compute_rule_statistics\n",
    "from rule_new_utils import check_r3_r2_batch, infer_rule_from_sample_batch\n",
    "\n",
    "class MultiIdxMambaModel(nn.Module):\n",
    "    def __init__(self, attribute_dims=(7,10,10), vocab_size=0, n_embd=768, n_class=0, is_sep_embed=True, **kwargs):\n",
    "        super().__init__()\n",
    "        # Combine embeddings\n",
    "        combined_embedding_size = n_embd  # Adjust based on your combination strategy\n",
    "        if is_sep_embed:\n",
    "            self.sep_word_embed = SepWordEmbed(attribute_dims, embed_size=n_embd//3)\n",
    "            self.multi_lmhead = SepLMhead(attribute_dims, embed_size=n_embd//3)\n",
    "        else:\n",
    "            self.sep_word_embed = CmbWordEmbed(attribute_dims, embed_size=n_embd)\n",
    "            self.multi_lmhead = CmbLMhead(attribute_dims, embed_size=n_embd)\n",
    "        config = MambaConfig(vocab_size=vocab_size, d_model=n_embd, **kwargs, \n",
    "                  ssm_cfg={\"layer\": \"Mamba1\"})\n",
    "        self.mamba = MambaLMHeadModel(config, device='cuda')\n",
    "        self.mamba.backbone.embedding = nn.Identity()\n",
    "        self.mamba.lm_head = nn.Identity()\n",
    "        self.context_embed = nn.Embedding(1+n_class, n_embd)\n",
    "\n",
    "    def forward(self, input_ids, y=None):\n",
    "        # input_ids is expected to be a list of three tensors [attr1, attr2, attr3]\n",
    "        if y is None:\n",
    "            y = torch.zeros(input_ids.shape[0], dtype=th.long).to(input_ids[0].device)\n",
    "        ctx_vec = self.context_embed(y)\n",
    "        combined_embedding = self.sep_word_embed(input_ids)\n",
    "        combined_embedding = torch.concat([ctx_vec[:,None,:], combined_embedding, ], dim=1)\n",
    "        outputs = self.mamba(combined_embedding) # this is actually hidden states not logits\n",
    "        logits_attr1, logits_attr2, logits_attr3 = self.multi_lmhead(outputs.logits)\n",
    "        return outputs, logits_attr1, logits_attr2, logits_attr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/n/home12/binxuwang/Github/DiffusionReasoning/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/n/home12/binxuwang/.local/lib/python3.10/site-packages\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mpath:\n",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/n/home12/binxuwang/Github/DiffusionReasoning/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/n/home12/binxuwang/.local/lib/python3.10/site-packages\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mpath:\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch2/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import sys\n",
    "sys.path.append('/n/home12/binxuwang/Github/DiffusionReasoning/')\n",
    "if '/n/home12/binxuwang/.local/lib/python3.10/site-packages' in sys.path:\n",
    "    sys.path.remove('/n/home12/binxuwang/.local/lib/python3.10/site-packages',)\n",
    "import os\n",
    "from os.path import join\n",
    "import time\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import einops\n",
    "import json\n",
    "# Import necessary libraries\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "# from torch.optim import AdamW\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "# from transformers import GPT2Model, GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "# from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "from GPT_models.GPT_RAVEN_model_lib import completion_eval, sample_next_token, \\\n",
    "    multi_attr_loss_vec, MultiIdxGPT2Model\n",
    "from GPT_models.GPT_RAVEN_model_lib import seqtsr2imgtsr, seqtsr2attrtsr, compute_rule_statistics\n",
    "from rule_new_utils import check_r3_r2_batch, infer_rule_from_sample_batch\n",
    "\n",
    "# Initialize the GPT-2 model and tokenizer\n",
    "def preprocess_ids(attr_seq_tsr, ):\n",
    "    attr_seq_tsr_pps = attr_seq_tsr.clone() + 1\n",
    "    return attr_seq_tsr_pps\n",
    "\n",
    "# heldout_id = [1, 16, 20, 34, 37]\n",
    "heldout_id = []\n",
    "# Create a mask with all True values\n",
    "# Set the specified rows to False\n",
    "train_mask = torch.ones(40, dtype=torch.bool)\n",
    "train_mask[heldout_id] = False\n",
    "data_dir = '/n/home12/binxuwang/Github/DiffusionReasoning/'\n",
    "attr_all = np.load(data_dir+'attr_all.npy')\n",
    "print(attr_all.shape)\n",
    "attr_all_rows = torch.tensor(attr_all)\n",
    "attr_img_tsr = einops.rearrange(attr_all_rows,  'class (B R) p (h w) attr -> class B attr (R h) (p w)', h=3,w=3,p=3,R=3)\n",
    "attr_img_tsr_train, attr_img_tsr_val = attr_img_tsr[train_mask, :3950], attr_img_tsr[:, 3950:] # changed June 30, 2024, also eval on untrained rules. \n",
    "attr_seq_tsr_train = einops.rearrange(attr_img_tsr_train,  'class B attr (R h) (p w) -> (class B) (R p h w) attr', h=3,w=3,p=3,R=3)\n",
    "attr_seq_tsr_val = einops.rearrange(attr_img_tsr_val,  'class B attr (R h) (p w) -> (class B) (R p h w) attr', h=3,w=3,p=3,R=3)\n",
    "attr_seq_tsr_train = preprocess_ids(attr_seq_tsr_train)\n",
    "attr_seq_tsr_val = preprocess_ids(attr_seq_tsr_val)\n",
    "print(attr_seq_tsr_train.shape, attr_seq_tsr_val.shape)\n",
    "\n",
    "saveroot = \"/n/holylfs06/LABS/kempner_fellow_binxuwang/Users/binxuwang/DL_Projects/Mamba_raven\"\n",
    "\n",
    "batch_size = 64\n",
    "epoch_total = 100\n",
    "save_ckpt_every = 5\n",
    "# explabel = \"Mamba_base_RAVEN_uncond_heldout0\"\n",
    "# n_embd = 768\n",
    "# n_layer = 12\n",
    "# is_sep_embed = True\n",
    "# explabel = \"mamba_medium_RAVEN_uncond_heldout0\"\n",
    "# explabel = \"mamba_medium_RAVEN_uncond_all\"\n",
    "# n_embd = 768\n",
    "# n_layer = 24\n",
    "# is_sep_embed = True\n",
    "# explabel = \"mamba_big_RAVEN_uncond_heldout0\"\n",
    "# explabel = \"mamba_big_RAVEN_uncond_all\"\n",
    "# n_embd = 1152\n",
    "# n_layer = 36\n",
    "# is_sep_embed = True\n",
    "# explabel = \"mamba_huge_RAVEN_uncond_heldout0\"\n",
    "explabel = \"mamba_huge_RAVEN_uncond_all\"\n",
    "n_embd = 1536\n",
    "n_layer = 48\n",
    "is_sep_embed = True\n",
    "# explabel = \"GPT2CmbEmb_base_RAVEN_uncond_heldout0\"\n",
    "# n_embd = 768\n",
    "# n_layer = 12\n",
    "# is_sep_embed = False\n",
    "n_class = 0\n",
    "lr = 1e-4\n",
    "num_warmup_steps = 100\n",
    "eval_temperature = 1.0\n",
    "\n",
    "expdir = join(saveroot, f\"{explabel}-{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "ckptdir = join(expdir, \"ckpt\")\n",
    "sampledir = join(expdir, \"samples\")\n",
    "for d in [expdir, ckptdir, sampledir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=join(expdir, 'tensorboard_logs'))\n",
    "\n",
    "config = {\"batch_size\": batch_size, \"epoch_total\": epoch_total, \"save_ckpt_every\": save_ckpt_every,\n",
    "           \"lr\": lr, \"num_warmup_steps\": num_warmup_steps,\n",
    "           \"n_embd\": n_embd, \"n_class\": n_class, \"n_layer\": n_layer, \n",
    "           \"is_sep_embed\": is_sep_embed,\n",
    "           \"heldout_id\": heldout_id, \n",
    "           \"train_sample_num\": len(attr_seq_tsr_train), \n",
    "           \"val_sample_num\": len(attr_seq_tsr_val),\n",
    "           \"eval_temperature\": eval_temperature}\n",
    "json.dump(config, open(join(expdir, \"config.json\"), 'w'))\n",
    "\n",
    "# bug fix @2024-06-28, before which, the \"n_embd\": n_embd, \"n_class\": n_class, \"n_layer\": n_layer, \"n_head\": n_head, are no effect\n",
    "mamba_raven = MultiIdxMambaModel(attribute_dims=(7,10,10), vocab_size=27, \n",
    "                               n_class=n_class, n_embd=n_embd, n_layer=n_layer, is_sep_embed=is_sep_embed)\n",
    "# train loop\n",
    "# dataset = torch.utils.data.TensorDataset(attr_seq_tsr_pps)\n",
    "data_loader = torch.utils.data.DataLoader(attr_seq_tsr_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(attr_seq_tsr_val, batch_size=256, shuffle=False, drop_last=False)\n",
    "\n",
    "num_training_steps = len(data_loader) * epoch_total\n",
    "optimizer = AdamW(mamba_raven.parameters(), lr=lr)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
    "mamba_raven.train().to('cuda')\n",
    "th.save(mamba_raven.state_dict(), join(ckptdir, 'mamba_init.pth'))\n",
    "global_step = 0\n",
    "for epoch in range(epoch_total):\n",
    "    mamba_raven.train()\n",
    "    pbar = tqdm(data_loader)\n",
    "    train_loss_sum = []\n",
    "    for step, inputs in enumerate(pbar):\n",
    "        inputs = inputs.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs, logits_attr1, logits_attr2, logits_attr3 = mamba_raven(inputs, y=None)\n",
    "        # note the inputs were pre-pended in gpt2 to add context\n",
    "        loss = multi_attr_loss_vec([logits_attr1[:,:-1], logits_attr2[:,:-1], logits_attr3[:,:-1]], \n",
    "                                   inputs)\n",
    "        # loss = next_token_loss((attr_seq_tsr_1, attr_seq_tsr_2, attr_seq_tsr_3), inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss_sum.append(loss.item())\n",
    "        pbar.set_description(f'Loss: {loss.item()} lr: {scheduler.get_last_lr()[0]}')\n",
    "        writer.add_scalar('Loss/train', loss.item(), global_step)\n",
    "        writer.add_scalar('lr', scheduler.get_last_lr()[0], global_step)\n",
    "        global_step += 1\n",
    "    \n",
    "    train_loss_avg = np.mean(train_loss_sum)\n",
    "    writer.add_scalar('Train/Avg_Loss', train_loss_avg, epoch)\n",
    "    mamba_raven.eval()\n",
    "    pbar = tqdm(val_loader)\n",
    "    val_loss_sum = []\n",
    "    for inputs in pbar:\n",
    "        inputs = inputs.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs, logits_attr1, logits_attr2, logits_attr3 = mamba_raven(inputs)\n",
    "            loss = multi_attr_loss_vec([logits_attr1[:,:-1], logits_attr2[:,:-1], logits_attr3[:,:-1]], \n",
    "                                       inputs)\n",
    "        # loss = next_token_loss((attr_seq_tsr_1, attr_seq_tsr_2, attr_seq_tsr_3), inputs)\n",
    "        pbar.set_description(f'Loss: {loss.item()}')\n",
    "        val_loss_sum.append(loss.item())\n",
    "    print(\"Validation Cross Entropy Loss \",np.mean(val_loss_sum))\n",
    "    writer.add_scalar('Val/Avg_Loss', np.mean(val_loss_sum), epoch)\n",
    "    # evaluatate on the completion of validation set\n",
    "    # rnd_idx = np.random.choice(len(attr_seq_tsr_val), 512)\n",
    "    eval_samples = attr_seq_tsr_val[:,:,:]\n",
    "    eval_complete, C3_list, C2_list, rule_col_list, stats = completion_eval(eval_samples, mamba_raven, num_mask=9, batch_size=256, \n",
    "                                                                     device='cuda', strategy=\"greedy\", return_stats=True)\n",
    "    # evaluation by ab initio generation of samples\n",
    "    eval_samples_empty = th.zeros(2048, 81, 3, dtype=th.long).to('cuda')\n",
    "    eval_complete_abinit, C3_list_abinit, C2_list_abinit, rule_col_list_abinit, stats_abinit = completion_eval(eval_samples_empty, mamba_raven, num_mask=81, batch_size=256, \n",
    "                                                device='cuda', strategy=\"sample\", temperature=eval_temperature, return_stats=True)\n",
    "    th.save({\"eval_complete\": eval_complete, \"C3_list\": C3_list, \"C2_list\": C2_list, \"rule_col_list\": rule_col_list, \"stats\": stats,\n",
    "             \"eval_complete_abinit\": eval_complete_abinit, \"C3_list_abinit\": C3_list_abinit, \"C2_list_abinit\": C2_list_abinit, \"rule_col_list_abinit\": rule_col_list_abinit, \"stats_abinit\": stats_abinit}, \n",
    "               join(sampledir, f\"eval_epoch{epoch}.pt\"))\n",
    "    writer.add_scalar('Val/C3', stats['C3'] / stats['total'], epoch)\n",
    "    writer.add_scalar('Val/C2', stats['C2'] / stats['total'], epoch)\n",
    "    writer.add_scalar('Val/AnyValid', stats['anyvalid'] / stats['total'] / 3, epoch)\n",
    "    writer.add_scalar('Val/C3_abinit', stats_abinit['C3'] / stats_abinit['total'], epoch)\n",
    "    writer.add_scalar('Val/C2_abinit', stats_abinit['C2'] / stats_abinit['total'], epoch)\n",
    "    writer.add_scalar('Val/AnyValid_abinit', stats_abinit['anyvalid'] / stats_abinit['total'] / 3, epoch)\n",
    "\n",
    "    if (epoch + 1) % save_ckpt_every == 0:\n",
    "        th.save(mamba_raven.state_dict(), join(ckptdir, f'mamba_ep{epoch}.pth'))\n",
    "\n",
    "th.save(mamba_raven.state_dict(), join(ckptdir, 'mamba_final.pth'))\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
